{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetPopularityPrediction-UsingDeepLearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "efpbbYlqHsHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Architecture:\n",
        "\n",
        "# 1. textual extractor:\n",
        "#     clean data\n",
        "#     create embedding vectors for each tweet using keras embedding layer\n",
        "#     feed embedding vectors to lstm to extract latent textual features\n",
        "\n",
        "# 2. Retweet count extractor:\n",
        "#     we already extracted number of retweets in a time slot. File: final_data_cleaned_lstm.csv\n",
        "#     use one-hot encoder to represent retweets in each time window to  a vector\n",
        "#     we now have a sequence of vectors for a sequence of time windows corresponding to each tweet.\n",
        "#     feed each embedding vectors to single gru to exract latent features.\n",
        "#     deploy attention layer after gru\n",
        "\n",
        "# We're not considering the 3rd module since we don't have the required data. Future possiblity of including follower count\n",
        "\n",
        "# 3. Final module:\n",
        "#     concatenate lstm and gru\n",
        "\n",
        "\n",
        "import contractions\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM,Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import concatenate,Dense\n",
        "from tensorflow.keras.layers import GRU,Dense,Attention,Multiply\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import Model\n",
        "import json\n",
        "import io\n",
        "import pandas as pd\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "np.random.seed(0)\n",
        "random.seed(9)\n",
        "tf.random.set_seed(123)\n",
        "from numpy import array\n",
        "import re\n",
        "import nltk\n",
        "from ast import literal_eval\n",
        "from sklearn.metrics import f1_score, log_loss, precision_recall_fscore_support\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zvo5tI3KNHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions\n",
        "def vocab_count(dataframe):\n",
        "    # Get pandas dataframe and calculate vocablury size\n",
        "    results = set()\n",
        "    dataframe['Text'].str.lower().str.split().apply(results.update)\n",
        "    return len(results)\n",
        "\n",
        "def retweet_class_count(data):\n",
        "    results = set()\n",
        "    for l in data:\n",
        "        results.update(l)\n",
        "    return max(results) + 1\n",
        "\n",
        "def data_length(data):\n",
        "    data = data.to_frame(name='text')\n",
        "    lengths = []\n",
        "    url_count = []\n",
        "    for _,row in data.iterrows():\n",
        "        text = row['text']\n",
        "        lengths.append(len(text))\n",
        "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "        url = re.findall(regex,text)\n",
        "        url_count.append(len(url))\n",
        "    return lengths,url_count\n",
        "\n",
        "def normalize(data): # min-max normalization of explicit features\n",
        "    min_data = min(data)\n",
        "    max_data = max(data)\n",
        "    diff = max_data-min_data\n",
        "    res = [round((float(i)-min_data)/diff,7) for i in data]\n",
        "    return res\n",
        "\n",
        "def data_cleaner(data):\n",
        "    \n",
        "    # Clean data and remove\n",
        "    # TODO\n",
        "    # This is neccessary as we're getting random numbers in final data. Couldn't find the cause.\n",
        "    data = data.to_frame(name='text')\n",
        "    data_cleaned = [] # List as we need it to be fed to model\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "    for _, row in data.iterrows():\n",
        "        text = row['text']\n",
        "        text = re.sub('([’.,!?()])\"\\'_', r' \\1 ', text) # add padding around punctutation marks\n",
        "        text = contractions.fix(text) # fix contractions like don't to do not\n",
        "        text = ''.join(filter(whitelist.__contains__, text)) # remove all non alpha characters\n",
        "        # needs removing extra data at end, html links, addiitonal spacing\n",
        "        text = text[:text.find(\"Name text dtype object\")] # Caused due to dtype in dataset\n",
        "        data_cleaned.append(text)\n",
        "    return data_cleaned\n",
        "\n",
        "def data_load(file_name):\n",
        "    data = pd.read_csv(file_name)\n",
        "    return data\n",
        "\n",
        "def str_to_arr(data):\n",
        "    result =[]\n",
        "    for d in data:\n",
        "        result.append(literal_eval(d))\n",
        "    return result\n",
        "\n",
        "def encode(arr,size):\n",
        "    res = [0] * size\n",
        "    for i in arr:\n",
        "        res[i] = 1\n",
        "    return res \n",
        "\n",
        "def save_model(model,model_path):\n",
        "    model_json = model.to_json()\n",
        "    with open(model_path,\"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "\n",
        "def train(model, \n",
        "          X_train, y_train,X_val,y_val, \n",
        "          checkpoint_path='model.hdf5', \n",
        "          epochs = 25,\n",
        "          steps_per_epoch=50, \n",
        "          batch_size = 32, \n",
        "          class_weights = None, \n",
        "          fit_verbose=1,\n",
        "          print_summary = True\n",
        "         ):\n",
        "    if print_summary:\n",
        "        print(model.summary())\n",
        "    model.fit(\n",
        "        X_train, \n",
        "        y_train, \n",
        "        #this is bad practice using test data for validation, in a real case would use a seperate validation set\n",
        "        validation_data=(X_val,y_val),  \n",
        "        epochs=epochs, \n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        class_weight=class_weights,\n",
        "         #saves the most accurate model, usually you would save the one with the lowest loss\n",
        "        \n",
        "        callbacks= [\n",
        "            ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True,mode='min'),    \n",
        "            EarlyStopping(patience = 2)\n",
        "        ],\n",
        "        verbose=fit_verbose\n",
        "    )\n",
        "    return model \n",
        "def test(model,X_test,y_test,checkpoint_path):\n",
        "\n",
        "    print('Loading Best Model...')\n",
        "    model.load_weights(checkpoint_path)\n",
        "    predictions = model.predict(X_test, verbose=1)\n",
        "    y_test_arr = np.asarray(y_test)\n",
        "    print('Test Loss:', log_loss(y_test_arr, predictions))\n",
        "    print('Test Accuracy', predictions.argmax(axis = 1) == y_test_arr.argmax(axis = 1))\n",
        "    precision,recall,f1,_ = precision_recall_fscore_support(y_test_arr.argmax(axis = 1), predictions.argmax(axis = 1), average='weighted')\n",
        "    print('Precision: {0}, Recall: {1},F1 Score: {2}'.format(precision,recall,f1))\n",
        "    \n",
        "    return model #returns best performing model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOgoZT4yKNFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read data\n",
        "f = \"dataset_final.csv\"\n",
        "data = data_load(f)\n",
        "data_ids = data['Id'].to_list()\n",
        "\n",
        "data_text = data_cleaner(data['Text'].astype('str'))\n",
        "f = 'explicit_features.csv'\n",
        "d = data_load(f)\n",
        "\n",
        "explicit_features = [list(row) for row in d.values]\n",
        "\n",
        "data_retweets = str_to_arr(data['Retweets'].to_list())\n",
        "data_retweets = [data[:6] for data in data_retweets] # with time_window = 10min and observation time = 1 hour\n",
        "\n",
        "vocab_size = vocab_count(data)\n",
        "retweet_class_size = retweet_class_count(data_retweets)\n",
        "\n",
        "tokenizer = Tokenizer(lower=False)\n",
        "tokenizer.fit_on_texts(data_text)\n",
        "\n",
        "# Uncomment for saving the tokenizer\n",
        "# tokenizer_json = tokenizer.to_json()\n",
        "# with io.open('tokenizer-store.json', 'w', encoding='utf-8') as f:\n",
        "#     f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "encoded_text = tokenizer.texts_to_sequences(data_text)\n",
        "max_pad_length = 25 # number of words to be padded\n",
        "padded_text = pad_sequences(encoded_text,maxlen = max_pad_length, padding = 'post') # done so that all sentences are of same size\n",
        "\n",
        "encoded_retweet_counts = np.array([to_categorical(d, num_classes = retweet_class_size) for d in data_retweets])\n",
        "\n",
        "labels = []\n",
        "d = pd.read_csv(\"final_data_cleaned_classification_with_classes.csv\")\n",
        "label_col = np.squeeze(d['48 hours'].to_list())\n",
        "\n",
        "thres = 700 # can be varied, class definition is based on it\n",
        "for t in label_col:\n",
        "    if t<thres:\n",
        "        labels.append(0)\n",
        "    else:\n",
        "        labels.append(1)\n",
        "labels = to_categorical(labels, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-duzBOBKNCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM model\n",
        "input_shape_texts = (padded_text.shape[1])\n",
        "input_textExt = Input(shape = input_shape_texts)\n",
        "tEl1 = Embedding(vocab_size,128,input_length=input_shape_texts)(input_textExt)\n",
        "tEl2 = LSTM(units=256,activation= 'relu',dropout=0.4)(tEl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYfy1p4kKM_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRU model\n",
        "input_shape = encoded_retweet_counts.shape\n",
        "input_shape_rCE = (input_shape[1],input_shape[2],)\n",
        "input_rCE = Input(shape=input_shape_rCE)\n",
        "rCEl1 = GRU(units=256,dropout=0.1,activation='relu')(input_rCE)\n",
        "attention_probs = Dense( units=256,activity_regularizer=regularizers.l2(0.01), activation='relu', name='attention_probs')(rCEl1)\n",
        "attention_mul = Multiply(name='attention_mul')([ rCEl1, attention_probs])\n",
        "rcEl2 = Dense(256, activity_regularizer=regularizers.l2(0.01), activation='sigmoid')(attention_mul)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onTM18-VKM9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explicit features\n",
        "input_shape_ef = len(explicit_features[0])\n",
        "input_ef = Input(shape=input_shape_ef)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfkjwIe8KM6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Final model\n",
        "merged1 = concatenate([tEl2,rcEl2])\n",
        "fl1 = Dense(256,activity_regularizer=regularizers.l2(0.01),activation='relu')(merged1)\n",
        "fl2 = Dense(256,activity_regularizer=regularizers.l2(0.01),activation='sigmoid')(fl1)\n",
        "merged2 = concatenate([fl2,input_ef])\n",
        "fl3 = Dense(2,activity_regularizer=regularizers.l2(0.01),activation='softmax')(merged2)\n",
        "model = Model(inputs=[input_textExt,input_rCE,input_ef],outputs=[fl3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX_vicM3KmBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment to save the model\n",
        "# save_model(model,\"./model.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsVRK2VlKgNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# randomly split of data into train,validation and test data\n",
        "x1_train = []\n",
        "x2_train = []\n",
        "x3_train = []\n",
        "y_train = []\n",
        "x1_val = []\n",
        "x2_val = []\n",
        "x3_val = []\n",
        "y_val = []\n",
        "x1_test = []\n",
        "x2_test = []\n",
        "x3_test = []\n",
        "y_test = []\n",
        "\n",
        "texts = []\n",
        "retweet_count = []\n",
        "for ind in range(len(padded_text)):\n",
        "    r = random.random()\n",
        "    if r < 0.4:\n",
        "        x1_train.append(padded_text[ind])\n",
        "        x2_train.append(encoded_retweet_counts[ind])\n",
        "        x3_train.append(explicit_features[ind])\n",
        "        y_train.append(labels[ind])\n",
        "\n",
        "    elif 0.4 <= r < 0.6:\n",
        "        x1_val.append(padded_text[ind])\n",
        "        x2_val.append(encoded_retweet_counts[ind])\n",
        "        x3_val.append(explicit_features[ind])\n",
        "        y_val.append(labels[ind])\n",
        "\n",
        "    else:\n",
        "        x1_test.append(padded_text[ind])\n",
        "        x2_test.append(encoded_retweet_counts[ind])\n",
        "        x3_test.append(explicit_features[ind])\n",
        "        y_test.append(labels[ind])\n",
        "        texts.append(data_text[ind])\n",
        "        retweet_count.append(label_col[ind])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuH7fsvAKgKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting to tensors\n",
        "x1_train = tf.convert_to_tensor(x1_train)\n",
        "x2_train = tf.convert_to_tensor(x2_train)\n",
        "x3_train = tf.convert_to_tensor(x3_train)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "x1_val = tf.convert_to_tensor(x1_val)\n",
        "x2_val = tf.convert_to_tensor(x2_val)\n",
        "x3_val = tf.convert_to_tensor(x3_val)\n",
        "y_val = tf.convert_to_tensor(y_val)\n",
        "x1_test = tf.convert_to_tensor(x1_test)\n",
        "x2_test = tf.convert_to_tensor(x2_test)\n",
        "x3_test = tf.convert_to_tensor(x3_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Af3U9MKgH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate =  0.01)\n",
        "model.compile(loss = 'binary_crossentropy',optimizer = optimizer, metrics =['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew54-lKrKgFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "model = train(model,checkpoint_path='./model_1-classify.h5',X_train=[x1_train,x2_train,x3_train],y_train=y_train,X_val = [x1_val,x2_val,x3_val],y_val= y_val,epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L_pMQIsK-Vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test\n",
        "model = test(model,checkpoint_path='./model_1-classify.h5',X_test = [x1_test,x2_test,x3_test],y_test=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}